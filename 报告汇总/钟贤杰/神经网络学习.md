```python
import torch
from torch import nn
from torch import optim
import torch.nn.functional as F
import torchvision
import os
import pandas as pd

print("yes")
```

    yes
    

# 向量

### 运算


```python
x = torch.tensor([3.0])
y = torch.tensor([2.0])
x + y, x * y, x / y, x**y
```




    (tensor([5.]), tensor([6.]), tensor([1.5000]), tensor([9.]))



### 创建


```python
 x = torch.arange(4)
x
```




    tensor([0, 1, 2, 3])



### 取值


```python
x[3]
```




    tensor(3.)



### 长度


```python
len(x)
```




    4



### 大小


```python
x.shape
```




    torch.Size([4])



# 矩阵

### 创建


```python
A = torch.arange(20).reshape(5,4)
A
```




    tensor([[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11],
            [12, 13, 14, 15],
            [16, 17, 18, 19]])




```python
X = torch.arange(24).reshape(2, 3, 4)
X
```




    tensor([[[ 0,  1,  2,  3],
             [ 4,  5,  6,  7],
             [ 8,  9, 10, 11]],
    
            [[12, 13, 14, 15],
             [16, 17, 18, 19],
             [20, 21, 22, 23]]])



### 转置


```python
A.T
```




    tensor([[ 0,  4,  8, 12, 16],
            [ 1,  5,  9, 13, 17],
            [ 2,  6, 10, 14, 18],
            [ 3,  7, 11, 15, 19]])



### 对称矩阵转置


```python
B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
B
```




    tensor([[1, 2, 3],
            [2, 0, 4],
            [3, 4, 5]])




```python
B == B.T
```




    tensor([[True, True, True],
            [True, True, True],
            [True, True, True]])



### 赋值/clone


```python
A = torch.arange(20, dtype=torch.float32).reshape(5,4)
B = A
B[:] = 2
A, B
```




    (tensor([[2., 2., 2., 2.],
             [2., 2., 2., 2.],
             [2., 2., 2., 2.],
             [2., 2., 2., 2.],
             [2., 2., 2., 2.]]),
     tensor([[2., 2., 2., 2.],
             [2., 2., 2., 2.],
             [2., 2., 2., 2.],
             [2., 2., 2., 2.],
             [2., 2., 2., 2.]]))




```python
A = torch.arange(20, dtype=torch.float32).reshape(5,4)
B = A.clone()
B[:] = 2
A, B
```




    (tensor([[ 0.,  1.,  2.,  3.],
             [ 4.,  5.,  6.,  7.],
             [ 8.,  9., 10., 11.],
             [12., 13., 14., 15.],
             [16., 17., 18., 19.]]),
     tensor([[2., 2., 2., 2.],
             [2., 2., 2., 2.],
             [2., 2., 2., 2.],
             [2., 2., 2., 2.],
             [2., 2., 2., 2.]]))



### 运算


```python
A * B
```




    tensor([[ 0.,  2.,  4.,  6.],
            [ 8., 10., 12., 14.],
            [16., 18., 20., 22.],
            [24., 26., 28., 30.],
            [32., 34., 36., 38.]])




```python
a = 2
X = torch.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
```




    (tensor([[[ 2,  3,  4,  5],
              [ 6,  7,  8,  9],
              [10, 11, 12, 13]],
     
             [[14, 15, 16, 17],
              [18, 19, 20, 21],
              [22, 23, 24, 25]]]),
     torch.Size([2, 3, 4]))




```python
x = torch.arange(4, dtype=torch.float32)
y = torch.ones(4, dtype=torch.float32)
x, y, torch.dot(x, y)
```




    (tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))




```python
torch.sum(x * y)
```




    tensor(6.)



### 求和


```python
x, x.sum()
```




    (tensor([0., 1., 2., 3.]), tensor(6.))




```python
A = torch.arange(2 * 2 * 20, dtype=torch.float32).reshape(2, 2, 5, 4)
A, A.shape, A.sum()
```




    (tensor([[[[ 0.,  1.,  2.,  3.],
               [ 4.,  5.,  6.,  7.],
               [ 8.,  9., 10., 11.],
               [12., 13., 14., 15.],
               [16., 17., 18., 19.]],
     
              [[20., 21., 22., 23.],
               [24., 25., 26., 27.],
               [28., 29., 30., 31.],
               [32., 33., 34., 35.],
               [36., 37., 38., 39.]]],
     
     
             [[[40., 41., 42., 43.],
               [44., 45., 46., 47.],
               [48., 49., 50., 51.],
               [52., 53., 54., 55.],
               [56., 57., 58., 59.]],
     
              [[60., 61., 62., 63.],
               [64., 65., 66., 67.],
               [68., 69., 70., 71.],
               [72., 73., 74., 75.],
               [76., 77., 78., 79.]]]]),
     torch.Size([2, 2, 5, 4]),
     tensor(3160.))




```python
A_sum_axis0 = A.sum(axis=0)
A_sum_axis0, A_sum_axis0.shape
```




    (tensor([[[ 40.,  42.,  44.,  46.],
              [ 48.,  50.,  52.,  54.],
              [ 56.,  58.,  60.,  62.],
              [ 64.,  66.,  68.,  70.],
              [ 72.,  74.,  76.,  78.]],
     
             [[ 80.,  82.,  84.,  86.],
              [ 88.,  90.,  92.,  94.],
              [ 96.,  98., 100., 102.],
              [104., 106., 108., 110.],
              [112., 114., 116., 118.]]]),
     torch.Size([2, 5, 4]))




```python
A.sum(axis=[0, 1])
```




    tensor(190.)



### 均值


```python
A.mean(), A.sum() / A.numel()
```




    (tensor(39.5000), tensor(39.5000))




```python
A.mean(axis=0), A.sum(axis=0) / A.shape[0]
```




    (tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))




```python
A.cumsum(axis=0)
```




    tensor([[ 0.,  1.,  2.,  3.],
            [ 4.,  6.,  8., 10.],
            [12., 15., 18., 21.],
            [24., 28., 32., 36.],
            [40., 45., 50., 55.]])



#### 广播机制


```python
sum_A = A.sum(axis=1, keepdims=True)
sum_NOKEEP_A = A.sum(axis=1)
sum_NOKEEP_A, sum_A
```




    (tensor([ 6., 22., 38., 54., 70.]),
     tensor([[ 6.],
             [22.],
             [38.],
             [54.],
             [70.]]))




```python
A / sum_A
```




    tensor([[0.0000, 0.1667, 0.3333, 0.5000],
            [0.1818, 0.2273, 0.2727, 0.3182],
            [0.2105, 0.2368, 0.2632, 0.2895],
            [0.2222, 0.2407, 0.2593, 0.2778],
            [0.2286, 0.2429, 0.2571, 0.2714]])




```python
A, x, A * x, torch.mv(A, x)
```




    (tensor([[ 0.,  1.,  2.,  3.],
             [ 4.,  5.,  6.,  7.],
             [ 8.,  9., 10., 11.],
             [12., 13., 14., 15.],
             [16., 17., 18., 19.]]),
     tensor([0., 1., 2., 3.]),
     tensor([[ 0.,  1.,  4.,  9.],
             [ 0.,  5., 12., 21.],
             [ 0.,  9., 20., 33.],
             [ 0., 13., 28., 45.],
             [ 0., 17., 36., 57.]]),
     tensor([ 14.,  38.,  62.,  86., 110.]))




```python
B = torch.ones(4, 3)
torch.mm(A, B)
```




    tensor([[ 6.,  6.,  6.],
            [22., 22., 22.],
            [38., 38., 38.],
            [54., 54., 54.],
            [70., 70., 70.]])



### 范数


```python
u = torch.tensor([3.0, -4.0])
torch.norm(u)
```




    tensor(5.)




```python
torch.abs(u).sum()
```




    tensor(7.)




```python
torch.norm(torch.ones(4, 9))
```




    tensor(6.)



亚导数 ![image.png](attachment:image-3.png)![image-2.png](attachment:image-2.png)

# 自动求导


```python
x = torch.arange(4.0)
x
```




    tensor([0., 1., 2., 3.])




```python
x.requires_grad_(True) # 等价于‘x = torch.arange(4.0, requires_grad=True)’
x.grad # 默认是None
```


```python
# y = 2x^2
y = 2 * torch.dot(x, x)
y
```




    tensor(28., grad_fn=<MulBackward0>)



# ???


```python
y.backward()
x.grad, x.grad == 4 * x
```




    (tensor([ 0.,  4.,  8., 12.]), tensor([True, True, True, True]))




```python
x.grad.zero_()
y = x.sum()
y
y.backward()
y, x.grad
```




    (tensor(6., grad_fn=<SumBackward0>), tensor([1., 1., 1., 1.]))



### 深度学习中，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和


```python
# 对非标量调用‘backward’需要传入一个‘gradient’参数，该参数指定微分函数
x.grad.zero_()
y = x * x
# 等价于y.backward(torch.ones(len(x)))
# y.backward(torch.ones(len(x)))
y.sum().backward()
x.grad
```




    tensor([0., 2., 4., 6.])




```python
x.grad.zero_()
y = x * x
u =  y.detach() # 标量
z = u * x

z.sum().backward()
x, y, u, z, z.sum(), x.grad == u
```




    (tensor([0., 1., 2., 3.], requires_grad=True),
     tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>),
     tensor([0., 1., 4., 9.]),
     tensor([ 0.,  1.,  8., 27.], grad_fn=<MulBackward0>),
     tensor(36., grad_fn=<SumBackward0>),
     tensor([True, True, True, True]))




```python
x.grad.zero_()
y.sum().backward()
x.grad == 2 * x, x.grad
```




    (tensor([True, True, True, True]), tensor([0., 2., 4., 6.]))



# 线性回归模型实现


```python
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
        print(b)
    if b.sum() > 0:
        c = b
    else:
        c = 100*b
    return c

a = torch.randn(size=(), requires_grad=True)
d = f(a)  
d.backward()
a, d , a.grad
```

    tensor(4.6266, grad_fn=<MulBackward0>)
    tensor(9.2533, grad_fn=<MulBackward0>)
    tensor(18.5066, grad_fn=<MulBackward0>)
    tensor(37.0131, grad_fn=<MulBackward0>)
    tensor(74.0263, grad_fn=<MulBackward0>)
    tensor(148.0526, grad_fn=<MulBackward0>)
    tensor(296.1051, grad_fn=<MulBackward0>)
    tensor(592.2102, grad_fn=<MulBackward0>)
    tensor(1184.4204, grad_fn=<MulBackward0>)
    




    (tensor(1.1567, requires_grad=True),
     tensor(1184.4204, grad_fn=<MulBackward0>),
     tensor(1024.))




```python
%matplotlib inline
import random
from d2l import torch as d2l
```


```python
def synthetic_data(w, b, num_examples):
    """生成 y = Xw + b + 噪声"""
    X = torch.normal(0, 1, (num_examples, len(w))) # 均值，标准差，（样本数量，列数）
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape) # 噪声
    return X, y.reshape((-1, 1))
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
```


```python
print('features:', features[[ 12, 837, 260, 633, 424, 744, 265, 425, 495, 480]], '\nlabel:', labels[0])
```

    features: tensor([[ 3.7911e-01,  2.0278e-01],
            [-5.9466e-01,  7.6497e-01],
            [-2.2224e+00,  1.4221e+00],
            [-1.1339e-01, -4.3873e-01],
            [ 8.7945e-05, -2.0094e-02],
            [ 2.5956e-01, -9.3418e-01],
            [ 7.7893e-01, -2.2530e+00],
            [ 1.9797e+00, -3.0954e+00],
            [-1.5709e-01, -1.5968e+00],
            [-5.5692e-01,  1.9005e-01]]) 
    label: tensor([-0.4956])
    


```python
d2l.set_figsize()
d2l.plt.scatter(features[:, 1].detach().numpy(),
               labels.detach().numpy(),1)
```




    <matplotlib.collections.PathCollection at 0x1f51a948610>




    
![svg](output_64_1.svg)
    



```python
def data_iter(batch_size, feature, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # 这些样本是随机读取的，没有特定顺序
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i:min(i + batch_size, num_examples)])    
        yield features[batch_indices], labels[batch_indices]
batch_size = 10
for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break
```

    tensor([[-0.3495, -1.1161],
            [-0.7504,  1.1402],
            [-0.0239,  0.2103],
            [ 0.4591,  0.8208],
            [ 1.5054,  1.3101],
            [ 0.2109,  0.5462],
            [ 0.4551,  0.9599],
            [ 0.1151, -2.0454],
            [-0.0592, -0.4876],
            [-1.0640, -1.0110]]) 
     tensor([[ 7.2894],
            [-1.1783],
            [ 3.4431],
            [ 2.3224],
            [ 2.7581],
            [ 2.7598],
            [ 1.8575],
            [11.3769],
            [ 5.7495],
            [ 5.5108]])
    

#### 定义初始化模型参数


```python
w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

#### 定义模型


```python
def linreg(X, w, b):
    """线性回归模型"""
    return torch.matmul(X, w) + b
```

#### 定义损失函数


```python
def squared_loss(y_hat, y):
    """均方损失"""
    return (y_hat - y.reshape(y_hat.shape))**2 / 2
```

#### 定义优化算法


```python
def sgd(params, lr, batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```


```python
lr = 0.02
num_epochs = 6
net = linreg
loss = squared_loss
for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y) # 'X'和'y'的小批量损失
        # 因为'l'形状是('batch_size', 1), 而不是一个标量
        # 并以此计算关于['w', 'b']的梯度
        l.sum().backward()
        sgd([w, b], lr, batch_size) # 使用参数的梯度更新参数
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch: {epoch + 1}, loss: {float(train_l.mean()):f}')
```

    epoch: 1, loss: 0.318551
    epoch: 2, loss: 0.006349
    epoch: 3, loss: 0.000177
    epoch: 4, loss: 0.000054
    epoch: 5, loss: 0.000052
    epoch: 6, loss: 0.000052
    


```python
print(f'w的估计误差：{true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差：{true_b - b}')
```

    w的估计误差：tensor([7.3791e-05, 4.6253e-05], grad_fn=<SubBackward0>)
    b的估计误差：tensor([0.0003], grad_fn=<RsubBackward1>)
    
